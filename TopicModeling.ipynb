{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in progress on finding hot trends in Reddit data sets. Here I employ probabilistic Topic Modeling, especially Latent Dirichlet Allocation (LDA). The goal is to find the trends over time by using moving average method to compute performance history of each topic within each subreddits and to use BM25 ranking method (state-of-the-art TF-IDF) to develop new trend ranking for the hot reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import operator\n",
    " \n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set() # For pretty plots\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = psycopg2.connect(database = 'Reddit', user = 'mrr-phys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subreddit_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>submission_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column_name\n",
       "0             id\n",
       "1   subreddit_id\n",
       "2  submission_id\n",
       "3        content"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of all the columns\n",
    "sql_query = \"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'main_comments';\n",
    "            \"\"\"\n",
    "col_list = pd.read_sql(sql_query, con)\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youtube',\n",
       " 'Best_Of_YouTube',\n",
       " 'youtube_recommended',\n",
       " 'funny',\n",
       " 'todayilearned',\n",
       " 'mildlyinteresting',\n",
       " 'announcements',\n",
       " 'aww',\n",
       " 'kiddet',\n",
       " 'justforkids',\n",
       " 'KidSafeVideos',\n",
       " 'childrensbooks',\n",
       " 'reallifedoodles',\n",
       " 'BeAmazed',\n",
       " 'Parenting',\n",
       " 'DoesAnybodyElse']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits = pd.read_sql(\"SELECT * FROM main_subreddits\", con)['name'].tolist()\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_submissions = pd.read_sql(\"SELECT * FROM main_submissions\", con)\n",
    "main_submissions = main_submissions.rename(columns = {'id': 'submission_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_comments = pd.read_sql(\"SELECT * FROM main_comments\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg = main_comments.groupby(['subreddit_id', 'submission_id'])['content'].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "dg = dg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>created</th>\n",
       "      <th>content_x</th>\n",
       "      <th>content_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6mfhvh</td>\n",
       "      <td>2qh44</td>\n",
       "      <td>1499732467</td>\n",
       "      <td></td>\n",
       "      <td>A bit more than a month ago I posted this: htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6mfayu</td>\n",
       "      <td>2qh44</td>\n",
       "      <td>1499730695</td>\n",
       "      <td>Hopefully you fine folks can help me remember ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6mf8x8</td>\n",
       "      <td>2qh44</td>\n",
       "      <td>1499730177</td>\n",
       "      <td>I have a playlist that is supposed to automati...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6mf61v</td>\n",
       "      <td>2qh44</td>\n",
       "      <td>1499729431</td>\n",
       "      <td>I mean seriously, The Youtube comment section ...</td>\n",
       "      <td>The YouTube comments system as a whole is so f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6mf402</td>\n",
       "      <td>2qh44</td>\n",
       "      <td>1499728902</td>\n",
       "      <td>There was a little magic wand next to my video...</td>\n",
       "      <td>The revert button in the video editor manager ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id subreddit_id     created  \\\n",
       "0        6mfhvh        2qh44  1499732467   \n",
       "1        6mfayu        2qh44  1499730695   \n",
       "2        6mf8x8        2qh44  1499730177   \n",
       "3        6mf61v        2qh44  1499729431   \n",
       "4        6mf402        2qh44  1499728902   \n",
       "\n",
       "                                           content_x  \\\n",
       "0                                                      \n",
       "1  Hopefully you fine folks can help me remember ...   \n",
       "2  I have a playlist that is supposed to automati...   \n",
       "3  I mean seriously, The Youtube comment section ...   \n",
       "4  There was a little magic wand next to my video...   \n",
       "\n",
       "                                           content_y  \n",
       "0  A bit more than a month ago I posted this: htt...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  The YouTube comments system as a whole is so f...  \n",
       "4  The revert button in the video editor manager ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.merge(main_submissions, dg, how='left', on=['subreddit_id', 'submission_id'])\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged['content'] = merged[['content_x', 'content_y']].astype(str).apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_submissions = merged['submission_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for ids in all_submissions:\n",
    "    data_local = merged[merged['submission_id'] == ids]['content'].tolist()\n",
    "    data.append(data_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "    u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "    u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "    u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "    u\"(\\ud83c[\\udde0-\\uddff])\"   # flags (iOS)\n",
    "    \"+\", flags = re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a post, cleans it and returns list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(document):\n",
    "    if type(document) == str: conv_post = unicode(document, \"utf-8\")\n",
    "    else: conv_post = document\n",
    "    document = emoji_pattern.sub('', conv_post)\n",
    "    document = re.sub(r'[^A-Za-z .-[\\d+]]+', '', document)\n",
    "    document = re.sub(r'\\b\\d+\\b', '', document)\n",
    "    document = document.replace('*', ' ')\n",
    "    document = document.replace('/', ' ')\n",
    "    document = document.replace('|', ' ')\n",
    "    document = document.replace('.', ' ')\n",
    "    document = document.replace('...', ' ')\n",
    "    document = document.replace('^', ' ')\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise(word):\n",
    "    #normalises words to lowercase and stems and lemmatizes it.\n",
    "    word = word.lower()\n",
    "    #word = lemmatizer.lemmatize(word)\n",
    "    #word = stemmer.stem(word)    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    tokened_data = [[normalise(word) for word in nltk.word_tokenize(clean_data(text)) if word not in string.punctuation] \n",
    "                    for doc in data for text in doc]\n",
    "    return tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_frequency(data):\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for doc in data:\n",
    "        for token in doc:\n",
    "            frequency[token] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_stopwords(frequency, h):\n",
    "    stops = set(sorted(frequency, key=frequency.get, reverse=True)[:h]).union(stopwords.words('english'))\n",
    "    stops = set(subreddits).union(stops)\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(doc, stops):\n",
    "    candidates_unigram = [token for token in doc if token not in stops]\n",
    "    #bigrams = nltk.ngrams(candidates_unigram,2)\n",
    "    #candidates_bigram = [\"_\".join(word) for word in bigrams]\n",
    "    return candidates_unigram #+ candidates_bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_infrequent_words(doc, frequency, l):\n",
    "    return [token for token in doc if frequency[token] > l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Preprocess data: ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_data = tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_frequency = get_words_frequency(tokenized_data)\n",
    "my_stops = extract_stopwords(words_frequency,300)\n",
    "ngrams_tokened_data = [extract_ngrams(tokened_doc, my_stops) for tokened_doc in tokenized_data]\n",
    "cleaned_docs= [remove_infrequent_words(tokened_doc, words_frequency, 30) for tokened_doc in ngrams_tokened_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Build LDA model ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in cleaned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=20, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = [[_ for _, word in lda.show_topic(topicno, topn=40)] for topicno in range(lda.num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: summer milk meal heat fruit pressure drink cut hot salt comfort neighborhood smoking beach plate healthy amazed set habit instagram small juice dreams plant super\n",
      "1: music song game movie amazon art tv games songs sound movies dp playing listen favorite original hear episode artist shows listening youtu voice alarm played\n",
      "2: girl boy birthday nice face na happy fuck dude cool awesome cute weird guys friend gif gon christmas head haha bike laugh super amazing nose\n",
      "3: world dinner org wikipedia women wiki en fucking american trump fuck fact men true news history hate country war pants sex white culture racist states\n",
      "4: app mobile click google page phone button site desktop feature apps screen res chrome option mode version moderators computer android issue works fix update working\n",
      "5: left top god clothes city picture damn tree beautiful chores fucking kitchen shop beer weed canada driver lady photo yep til memory painful leg mr\n",
      "6: words word english bathroom language glass signs french names swimming write speak german writing japanese baseball star boat letters japan sign called means mirror understand\n",
      "7: information law legal company bath fair case public rules allowed abuse support email report court request copyright order private situation illegal contact claim system policy\n",
      "8: children books mom dad parent bed age mother daycare parenting father reading doctor wife loves bedtime young loved sister house story asleep older stories fun\n",
      "9: car light cars black eyes camera white road snow speed street crib blue space hit red cool high source dark frame side wind earth fast\n",
      "10: posts karma front mods calm admins mod internet top popular site upvotes fuck posted upvote agree password porn page subreddit gif rule discussion account current\n",
      "11: page facebook link search cream gold front communities media title google social filter links chair pictures twitter add pages hey tag base results share user\n",
      "12: subs users ads account ad community views user news site banned channels spam accent mods popular posts algorithm advertisers free platform google subscribers revenue accounts\n",
      "13: hours job week month pay house weeks drive working sleeping buy seat spend toys worked hour small couple worth store college area shoes car care\n",
      "14: husband phone call hate red open number calls wet office eats green finger blue votes meds moderator yelling cards orange easier alien holding wait fingers\n",
      "15: subreddits png bot np message subreddit info compose wiki gif link context= thread reply top index nsfw image users jpg user source adhd youtu contact\n",
      "16: water potty clean walk body door hand hands mouth babies toilet wear training poop hair air floor feet cold pee room mental dry face sit\n",
      "17: talk friends room sounds understand advice learn told talking normal behavior experience attention feeling age wo situation older wife friend wanted minutes mind toddler stay\n",
      "18: eat food dog eating cat dogs pizza cheese fish cats foods butter chicken ice feeding activities vegan feed smell fat cook chocolate lunch hot snack\n",
      "19: nap animals meat buy bags table animal taste store meals humans snacks hungry waking breakfast human test food shopping preschool free bag product products teachers\n"
     ]
    }
   ],
   "source": [
    "for topicno, words in enumerate(top_words):\n",
    "    print(\"%i: %s\" % (topicno, ' '.join(words[:25])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*** Evaluation, Diagnostics and Improvements of Topics: ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pointwise mutual information (PMI):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log conditional probability (LCP):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
